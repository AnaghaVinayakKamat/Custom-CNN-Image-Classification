{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe91e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\anagh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\anagh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\anagh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\anagh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\anagh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\anagh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\anagh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\anagh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a5ad9",
   "metadata": {},
   "source": [
    "# Implentation in pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2a1eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature map shape: torch.Size([2, 3, 4, 4])\n",
      "Output feature map shape: torch.Size([2, 3, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anagh\\AppData\\Local\\Temp\\ipykernel_8300\\1408286135.py:37: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:77.)\n",
      "  input_feature_map = torch.randn(batch_size, num_channels, input_size[0], input_size[1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1508,  0.4774],\n",
       "          [ 2.7049,  3.7047]],\n",
       "\n",
       "         [[ 1.4852,  0.0697],\n",
       "          [ 1.4128,  0.0524]],\n",
       "\n",
       "         [[ 4.3981,  2.9169],\n",
       "          [ 2.4711,  0.4436]]],\n",
       "\n",
       "\n",
       "        [[[-1.1133,  2.5087],\n",
       "          [-1.3157, -0.0771]],\n",
       "\n",
       "         [[ 2.8690,  1.4631],\n",
       "          [-3.0108, -2.0201]],\n",
       "\n",
       "         [[ 1.8569,  1.8061],\n",
       "          [ 0.3483, -2.1285]]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class ConvolutionalLayer(nn.Module):\n",
    "    def __init__(self, input_size, num_channels, filter_size):\n",
    "        super(ConvolutionalLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_channels = num_channels\n",
    "        self.filter_size = filter_size\n",
    "        #make weights matrix the same size as the input\n",
    "        self.weight_matrix = nn.Parameter(torch.randn(batch_size, num_channels, input_size[0], input_size[1]))\n",
    "        self.output_size = (input_size[0] - filter_size[0] + 1, input_size[1] - filter_size[1] + 1)\n",
    "        self.output_feature_map = torch.zeros((self.num_channels, self.output_size[0], self.output_size[1]))\n",
    "\n",
    "    def forward(self, input_feature_map):\n",
    "        batch_size = input_feature_map.size(0)\n",
    "        output_feature_maps = []\n",
    "        for i in range(batch_size):\n",
    "            output_feature_map = torch.zeros((self.num_channels, self.output_size[0], self.output_size[1]))\n",
    "            for k in range(self.num_channels):\n",
    "                for j in range(self.output_size[0]):\n",
    "                    for l in range(self.output_size[1]):\n",
    "                        #the same receptive field is applied to the weights as the input\n",
    "                        receptive_field = input_feature_map[i, :, j:j+self.filter_size[0], l:l+self.filter_size[1]]\n",
    "                        receptive_field_weight = self.weight_matrix[i, :, j:j+self.filter_size[0], l:l+self.filter_size[1]]\n",
    "                        weighted_output = torch.sum(receptive_field * receptive_field_weight, dim=(1,2))\n",
    "                        output_feature_map[k, j, l] = weighted_output[k]\n",
    "            output_feature_maps.append(output_feature_map)\n",
    "        output_feature_maps = torch.stack(output_feature_maps, dim=0)\n",
    "        return output_feature_maps\n",
    "\n",
    "    \n",
    "batch_size = 2\n",
    "num_channels = 3\n",
    "input_size = (4, 4)\n",
    "input_feature_map = torch.randn(batch_size, num_channels, input_size[0], input_size[1])\n",
    "\n",
    "# Create a ConvolutionalLayer instance\n",
    "conv_layer = ConvolutionalLayer(input_size, num_channels, filter_size=(3, 3))\n",
    "\n",
    "# Forward pass\n",
    "output_feature_map = conv_layer(input_feature_map)\n",
    "\n",
    "# Print the shapes of the input and output feature maps\n",
    "print(\"Input feature map shape:\", input_feature_map.shape)\n",
    "print(\"Output feature map shape:\", output_feature_map.shape)\n",
    "output_feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41897d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature map shape: torch.Size([2, 3, 4, 4])\n",
      "Output feature map shape: torch.Size([2, 3, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.8636,  1.4562],\n",
       "          [-0.6605,  3.6737]],\n",
       "\n",
       "         [[-4.9924,  0.9139],\n",
       "          [-4.3014, -3.7409]],\n",
       "\n",
       "         [[-3.0257, -2.5399],\n",
       "          [-1.0523,  0.1142]]],\n",
       "\n",
       "\n",
       "        [[[-0.4047, -0.4929],\n",
       "          [-1.1061, -0.7353]],\n",
       "\n",
       "         [[-5.3033, -2.1761],\n",
       "          [-7.9218, -7.3981]],\n",
       "\n",
       "         [[-1.5904, -1.9801],\n",
       "          [-3.1135, -3.3438]]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class ConvolutionalLayer(nn.Module):\n",
    "    def __init__(self, input_size, num_channels, filter_size):\n",
    "        super(ConvolutionalLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_channels = num_channels\n",
    "        self.filter_size = filter_size\n",
    "        #make weights matrix the same size as the input\n",
    "        self.weight_matrix = nn.Parameter(torch.randn(batch_size, num_channels, input_size[0], input_size[1]))\n",
    "        self.output_size = (input_size[0] - filter_size[0] + 1, input_size[1] - filter_size[1] + 1)\n",
    "        self.output_feature_map = torch.zeros((self.num_channels, self.output_size[0], self.output_size[1]))\n",
    "\n",
    "    def forward(self, input_feature_map):\n",
    "        batch_size = input_feature_map.size(0)\n",
    "        output_feature_maps = []\n",
    "        for i in range(batch_size):\n",
    "            output_feature_map = torch.zeros((self.num_channels, self.output_size[0], self.output_size[1]))\n",
    "            for k in range(self.num_channels):\n",
    "                for j in range(self.output_size[0]):\n",
    "                    for l in range(self.output_size[1]):\n",
    "                        #the same receptive field is applied to the weights as the input\n",
    "                        receptive_field = input_feature_map[i, :, j:j+self.filter_size[0], l:l+self.filter_size[1]]\n",
    "                        receptive_field_weight = self.weight_matrix[i, :, j:j+self.filter_size[0], l:l+self.filter_size[1]]\n",
    "                        weighted_output = torch.sum(receptive_field * receptive_field_weight, dim=(1,2))\n",
    "                        output_feature_map[k, j, l] = weighted_output[k]\n",
    "            output_feature_maps.append(output_feature_map)\n",
    "        output_feature_maps = torch.stack(output_feature_maps, dim=0)\n",
    "        return output_feature_maps\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        batch_size = grad_output.size(0)\n",
    "        grad_input = torch.zeros((batch_size, self.input_size[0], self.input_size[1], self.filter_size[0], self.filter_size[1]), device=self.weight_matrix.device)\n",
    "        grad_weight = torch.zeros_like(self.weight_matrix)\n",
    "        for i in range(batch_size):\n",
    "            for k in range(self.num_channels):\n",
    "                for j in range(self.output_size[0]):\n",
    "                    for l in range(self.output_size[1]):\n",
    "                        # compute the gradient of the output w.r.t. the receptive field\n",
    "                        grad_weight[k] += grad_output[i, k, j, l] * self.input_feature_map[i, :, j:j+self.filter_size[0], l:l+self.filter_size[1]]\n",
    "                        # compute the gradient of the output w.r.t. the input feature map\n",
    "                        grad_input[i, :, j:j+self.filter_size[0], l:l+self.filter_size[1]] += grad_output[i, k, j, l] * self.weight_matrix[k]\n",
    "        self.weight_matrix.grad = torch.sum(grad_weight, dim=0, keepdim=True)\n",
    "        return grad_input\n",
    "\n",
    "    \n",
    "batch_size = 2\n",
    "num_channels = 3\n",
    "input_size = (4, 4)\n",
    "input_feature_map = torch.randn(batch_size, num_channels, input_size[0], input_size[1])\n",
    "\n",
    "# Create a ConvolutionalLayer instance\n",
    "conv_layer = ConvolutionalLayer(input_size, num_channels, filter_size=(3, 3))\n",
    "\n",
    "# Forward pass\n",
    "output_feature_map = conv_layer(input_feature_map)\n",
    "\n",
    "# Print the shapes of the input and output feature maps\n",
    "print(\"Input feature map shape:\", input_feature_map.shape)\n",
    "print(\"Output feature map shape:\", output_feature_map.shape)\n",
    "output_feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987ada3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4c117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
